{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAILResearch/AI_Teammates_in_SE3/blob/main/analysis/load_AIDev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa6TVe74DGQ8"
      },
      "source": [
        "## Análise de Ferramentas de IA para Desenvolvimento\n",
        "### Claude Code, Copilot e Cursor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Funções de Carregamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_json_file(filepath):\n",
        "    \"\"\"Carrega arquivo JSON, retorna {} se vazio ou inválido\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            return data if data else {}\n",
        "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
        "        print(f\"Erro ao carregar {filepath}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def load_tool_data(tool_path):\n",
        "    \"\"\"Carrega todos os dados de uma ferramenta\"\"\"\n",
        "    tool_path = Path(tool_path)\n",
        "    \n",
        "    data = {\n",
        "        'prs_json': load_json_file(tool_path / 'prs.json'),\n",
        "        'pr_commits': load_json_file(tool_path / 'pr_commits.json'),\n",
        "        'pr_reviews': load_json_file(tool_path / 'pr_reviews.json'),\n",
        "        'pr_review_comments': load_json_file(tool_path / 'pr_review_comments.json'),\n",
        "        'pr_comments': load_json_file(tool_path / 'pr_comments.json'),\n",
        "        'pr_timelines': load_json_file(tool_path / 'pr_timelines.json'),\n",
        "        'issues': load_json_file(tool_path / 'issues.json'),\n",
        "        'developer_metadata': load_json_file(tool_path / 'developer_metadata.json'),\n",
        "        'repo_metadata': load_json_file(tool_path / 'repo_metadata.json'),\n",
        "    }\n",
        "    \n",
        "    # Carrega CSVs\n",
        "    try:\n",
        "        data['prs_csv'] = pd.read_csv(tool_path / 'prs.csv')\n",
        "    except:\n",
        "        data['prs_csv'] = pd.DataFrame()\n",
        "    \n",
        "    try:\n",
        "        data['conventional_commits'] = pd.read_csv(tool_path / 'gpt_conventional_commits.csv')\n",
        "    except:\n",
        "        data['conventional_commits'] = pd.DataFrame()\n",
        "    \n",
        "    try:\n",
        "        data['related_issues'] = pd.read_csv(tool_path / 'related_issues.csv')\n",
        "    except:\n",
        "        data['related_issues'] = pd.DataFrame()\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Funções de Análise de Métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_datetime(dt_str):\n",
        "    \"\"\"Converte string de data para datetime\"\"\"\n",
        "    if pd.isna(dt_str) or dt_str == '':\n",
        "        return None\n",
        "    try:\n",
        "        return pd.to_datetime(dt_str)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def calculate_feedback_loop_metrics(data):\n",
        "    \"\"\"Calcula métricas de feedback loop\"\"\"\n",
        "    metrics = {\n",
        "        'tempo_ate_merge_hours': [],\n",
        "        'numero_revisoes': [],\n",
        "        'comentarios_ferramenta': [],\n",
        "        'tempo_primeira_revisao_hours': [],\n",
        "        'review_time_hours': []\n",
        "    }\n",
        "    \n",
        "    # Processa cada PR\n",
        "    prs = data['prs_json']\n",
        "    if isinstance(prs, list):\n",
        "        pr_list = prs\n",
        "    else:\n",
        "        pr_list = list(prs.values())\n",
        "    \n",
        "    for pr in pr_list:\n",
        "        if not isinstance(pr, dict):\n",
        "            continue\n",
        "            \n",
        "        pr_id = str(pr.get('id', ''))\n",
        "        \n",
        "        # Tempo até o merge\n",
        "        created = parse_datetime(pr.get('created_at'))\n",
        "        merged = parse_datetime(pr.get('pull_request', {}).get('merged_at')) if 'pull_request' in pr else parse_datetime(pr.get('merged_at'))\n",
        "        \n",
        "        if created and merged:\n",
        "            time_to_merge = (merged - created).total_seconds() / 3600\n",
        "            metrics['tempo_ate_merge_hours'].append(time_to_merge)\n",
        "        \n",
        "        # Número de revisões\n",
        "        reviews = data['pr_reviews'].get(f\"{pr_id}.json\", [])\n",
        "        if isinstance(reviews, list):\n",
        "            metrics['numero_revisoes'].append(len(reviews))\n",
        "            \n",
        "            # Tempo até primeira revisão\n",
        "            if reviews and created:\n",
        "                first_review_time = parse_datetime(reviews[0].get('submitted_at'))\n",
        "                if first_review_time:\n",
        "                    time_to_first_review = (first_review_time - created).total_seconds() / 3600\n",
        "                    metrics['tempo_primeira_revisao_hours'].append(time_to_first_review)\n",
        "        \n",
        "        # Comentários da ferramenta (incluindo review comments)\n",
        "        review_comments = data['pr_review_comments'].get(f\"{pr_id}.json\", [])\n",
        "        pr_comments = data['pr_comments'].get(f\"{pr_id}.json\", [])\n",
        "        \n",
        "        total_comments = 0\n",
        "        if isinstance(review_comments, list):\n",
        "            total_comments += len(review_comments)\n",
        "        if isinstance(pr_comments, list):\n",
        "            total_comments += len(pr_comments)\n",
        "        \n",
        "        metrics['comentarios_ferramenta'].append(total_comments)\n",
        "        \n",
        "        # Review time (tempo desde criação até última revisão)\n",
        "        if reviews and isinstance(reviews, list) and created:\n",
        "            last_review_time = parse_datetime(reviews[-1].get('submitted_at'))\n",
        "            if last_review_time:\n",
        "                review_time = (last_review_time - created).total_seconds() / 3600\n",
        "                metrics['review_time_hours'].append(review_time)\n",
        "    \n",
        "    # Calcula estatísticas\n",
        "    result = {}\n",
        "    for key, values in metrics.items():\n",
        "        if values:\n",
        "            result[f\"{key}_mean\"] = np.mean(values)\n",
        "            result[f\"{key}_median\"] = np.median(values)\n",
        "            result[f\"{key}_std\"] = np.std(values)\n",
        "            result[f\"{key}_total\"] = len(values)\n",
        "        else:\n",
        "            result[f\"{key}_mean\"] = 0\n",
        "            result[f\"{key}_median\"] = 0\n",
        "            result[f\"{key}_std\"] = 0\n",
        "            result[f\"{key}_total\"] = 0\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_cognitive_load_metrics(data):\n",
        "    \"\"\"Calcula métricas de carga cognitiva\"\"\"\n",
        "    metrics = {\n",
        "        'conventional_commits': 0,\n",
        "        'total_comments': 0,\n",
        "        'issues_antes': 0,\n",
        "        'issues_depois': 0,\n",
        "        'frequencia_interrupcoes': [],\n",
        "        'arquivos_modificados': [],\n",
        "        'code_churn': []\n",
        "    }\n",
        "    \n",
        "    # Conventional commits\n",
        "    if not data['conventional_commits'].empty:\n",
        "        metrics['conventional_commits'] = len(data['conventional_commits'])\n",
        "    \n",
        "    # Total de comentários\n",
        "    for pr_comments in data['pr_comments'].values():\n",
        "        if isinstance(pr_comments, list):\n",
        "            metrics['total_comments'] += len(pr_comments)\n",
        "    \n",
        "    for review_comments in data['pr_review_comments'].values():\n",
        "        if isinstance(review_comments, list):\n",
        "            metrics['total_comments'] += len(review_comments)\n",
        "    \n",
        "    # Issues (tentativa de separar por tempo - antes e depois da ferramenta)\n",
        "    issues = data['issues']\n",
        "    if issues:\n",
        "        for issue in issues.values():\n",
        "            if isinstance(issue, dict):\n",
        "                created = parse_datetime(issue.get('created_at'))\n",
        "                closed = parse_datetime(issue.get('closed_at'))\n",
        "                \n",
        "                # Heurística simples: issues abertas = antes, fechadas = depois\n",
        "                if issue.get('state') == 'open':\n",
        "                    metrics['issues_antes'] += 1\n",
        "                elif closed:\n",
        "                    metrics['issues_depois'] += 1\n",
        "    \n",
        "    # Frequência de interrupções e arquivos modificados\n",
        "    for pr_id, commits in data['pr_commits'].items():\n",
        "        if isinstance(commits, list):\n",
        "            # Frequência de interrupções (tempo entre commits)\n",
        "            commit_times = []\n",
        "            files_in_pr = set()\n",
        "            \n",
        "            for commit in commits:\n",
        "                if isinstance(commit, dict):\n",
        "                    commit_info = commit.get('commit', {})\n",
        "                    author_info = commit_info.get('author', {})\n",
        "                    commit_time = parse_datetime(author_info.get('date'))\n",
        "                    \n",
        "                    if commit_time:\n",
        "                        commit_times.append(commit_time)\n",
        "                    \n",
        "                    # Contagem de arquivos (estimativa via commit message)\n",
        "                    message = commit_info.get('message', '')\n",
        "                    # Conta linhas de arquivo mencionadas ou estimativa\n",
        "                    files_mentioned = message.count('/')\n",
        "                    if files_mentioned > 0:\n",
        "                        files_in_pr.add(files_mentioned)\n",
        "            \n",
        "            # Calcula intervalo entre commits\n",
        "            if len(commit_times) > 1:\n",
        "                commit_times.sort()\n",
        "                intervals = []\n",
        "                for i in range(1, len(commit_times))\n",
        "                    interval = (commit_times[i] - commit_times[i-1]).total_seconds() / 3600\n",
        "                    intervals.append(interval)\n",
        "                if intervals:\n",
        "                    metrics['frequencia_interrupcoes'].append(np.mean(intervals))\n",
        "            \n",
        "            if files_in_pr:\n",
        "                metrics['arquivos_modificados'].append(len(files_in_pr))\n",
        "            \n",
        "            # Code churn (número de commits como proxy)\n",
        "            metrics['code_churn'].append(len(commits))\n",
        "    \n",
        "    # Calcula estatísticas\n",
        "    result = {\n",
        "        'conventional_commits_total': metrics['conventional_commits'],\n",
        "        'total_comments': metrics['total_comments'],\n",
        "        'issues_antes': metrics['issues_antes'],\n",
        "        'issues_depois': metrics['issues_depois'],\n",
        "        'issues_delta': metrics['issues_depois'] - metrics['issues_antes'],\n",
        "    }\n",
        "    \n",
        "    if metrics['frequencia_interrupcoes']:\n",
        "        result['frequencia_interrupcoes_mean_hours'] = np.mean(metrics['frequencia_interrupcoes'])\n",
        "        result['frequencia_interrupcoes_median_hours'] = np.median(metrics['frequencia_interrupcoes'])\n",
        "    else:\n",
        "        result['frequencia_interrupcoes_mean_hours'] = 0\n",
        "        result['frequencia_interrupcoes_median_hours'] = 0\n",
        "    \n",
        "    if metrics['arquivos_modificados']:\n",
        "        result['arquivos_modificados_mean'] = np.mean(metrics['arquivos_modificados'])\n",
        "        result['arquivos_modificados_median'] = np.median(metrics['arquivos_modificados'])\n",
        "    else:\n",
        "        result['arquivos_modificados_mean'] = 0\n",
        "        result['arquivos_modificados_median'] = 0\n",
        "    \n",
        "    if metrics['code_churn']:\n",
        "        result['code_churn_mean'] = np.mean(metrics['code_churn'])\n",
        "        result['code_churn_median'] = np.median(metrics['code_churn'])\n",
        "    else:\n",
        "        result['code_churn_mean'] = 0\n",
        "        result['code_churn_median'] = 0\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_flow_metrics(data):\n",
        "    \"\"\"Calcula métricas de flow\"\"\"\n",
        "    metrics = {\n",
        "        'total_prs': 0,\n",
        "        'prs_merged': 0,\n",
        "        'prs_closed': 0,\n",
        "        'prs_open': 0,\n",
        "        'tempo_entre_commits': [],\n",
        "        'tempo_ate_merge': []\n",
        "    }\n",
        "    \n",
        "    # Contagem de PRs\n",
        "    prs = data['prs_json']\n",
        "    if isinstance(prs, list):\n",
        "        pr_list = prs\n",
        "    else:\n",
        "        pr_list = list(prs.values())\n",
        "    \n",
        "    metrics['total_prs'] = len(pr_list)\n",
        "    \n",
        "    for pr in pr_list:\n",
        "        if not isinstance(pr, dict):\n",
        "            continue\n",
        "        \n",
        "        state = pr.get('state', '')\n",
        "        if state == 'open':\n",
        "            metrics['prs_open'] += 1\n",
        "        elif state == 'closed':\n",
        "            metrics['prs_closed'] += 1\n",
        "            \n",
        "            # Verifica se foi merged\n",
        "            merged_at = None\n",
        "            if 'pull_request' in pr:\n",
        "                merged_at = pr['pull_request'].get('merged_at')\n",
        "            else:\n",
        "                merged_at = pr.get('merged_at')\n",
        "            \n",
        "            if merged_at:\n",
        "                metrics['prs_merged'] += 1\n",
        "                \n",
        "                # Tempo até merge\n",
        "                created = parse_datetime(pr.get('created_at'))\n",
        "                merged = parse_datetime(merged_at)\n",
        "                if created and merged:\n",
        "                    time_to_merge = (merged - created).total_seconds() / 3600\n",
        "                    metrics['tempo_ate_merge'].append(time_to_merge)\n",
        "    \n",
        "    # Tempo entre commits (global)\n",
        "    all_commit_times = []\n",
        "    for commits in data['pr_commits'].values():\n",
        "        if isinstance(commits, list):\n",
        "            for commit in commits:\n",
        "                if isinstance(commit, dict):\n",
        "                    commit_info = commit.get('commit', {})\n",
        "                    author_info = commit_info.get('author', {})\n",
        "                    commit_time = parse_datetime(author_info.get('date'))\n",
        "                    if commit_time:\n",
        "                        all_commit_times.append(commit_time)\n",
        "    \n",
        "    if len(all_commit_times) > 1:\n",
        "        all_commit_times.sort()\n",
        "        for i in range(1, len(all_commit_times))\n",
        "            interval = (all_commit_times[i] - all_commit_times[i-1]).total_seconds() / 3600\n",
        "            metrics['tempo_entre_commits'].append(interval)\n",
        "    \n",
        "    # Calcula estatísticas\n",
        "    result = {\n",
        "        'total_prs': metrics['total_prs'],\n",
        "        'prs_open': metrics['prs_open'],\n",
        "        'prs_closed': metrics['prs_closed'],\n",
        "        'prs_merged': metrics['prs_merged'],\n",
        "        'merge_rate': metrics['prs_merged'] / metrics['total_prs'] if metrics['total_prs'] > 0 else 0,\n",
        "    }\n",
        "    \n",
        "    if metrics['tempo_entre_commits']:\n",
        "        result['tempo_entre_commits_mean_hours'] = np.mean(metrics['tempo_entre_commits'])\n",
        "        result['tempo_entre_commits_median_hours'] = np.median(metrics['tempo_entre_commits'])\n",
        "    else:\n",
        "        result['tempo_entre_commits_mean_hours'] = 0\n",
        "        result['tempo_entre_commits_median_hours'] = 0\n",
        "    \n",
        "    if metrics['tempo_ate_merge']:\n",
        "        result['tempo_ate_merge_mean_hours'] = np.mean(metrics['tempo_ate_merge'])\n",
        "        result['tempo_ate_merge_median_hours'] = np.median(metrics['tempo_ate_merge'])\n",
        "    else:\n",
        "        result['tempo_ate_merge_mean_hours'] = 0\n",
        "        result['tempo_ate_merge_median_hours'] = 0\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_profile_metrics(data):\n",
        "    \"\"\"Extrai perfil de desenvolvedores e projetos\"\"\"\n",
        "    profile = {\n",
        "        'num_developers': 0,\n",
        "        'num_repos': 0,\n",
        "        'languages': [],\n",
        "        'total_stars': 0,\n",
        "        'total_forks': 0,\n",
        "    }\n",
        "    \n",
        "    # Desenvolvedores\n",
        "    if data['developer_metadata']:\n",
        "        profile['num_developers'] = len(data['developer_metadata'])\n",
        "    \n",
        "    # Repositórios\n",
        "    if data['repo_metadata']:\n",
        "        profile['num_repos'] = len(data['repo_metadata'])\n",
        "        \n",
        "        for repo in data['repo_metadata'].values():\n",
        "            if isinstance(repo, dict):\n",
        "                lang = repo.get('language')\n",
        "                if lang:\n",
        "                    profile['languages'].append(lang)\n",
        "                \n",
        "                stars = repo.get('stargazers_count', 0)\n",
        "                forks = repo.get('forks_count', 0)\n",
        "                profile['total_stars'] += stars\n",
        "                profile['total_forks'] += forks\n",
        "    \n",
        "    # Linguagens únicas\n",
        "    profile['unique_languages'] = len(set(profile['languages']))\n",
        "    profile['primary_language'] = max(set(profile['languages']), key=profile['languages'].count) if profile['languages'] else 'N/A'\n",
        "    \n",
        "    return profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carregamento e Análise dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Caminhos das ferramentas\n",
        "tools = {\n",
        "    'Claude_Code': './claude_code',\n",
        "    'Copilot': './copilot',\n",
        "    'Cursor': './cursor'\n",
        "}\n",
        "\n",
        "# Carrega dados de todas as ferramentas\n",
        "all_data = {}\n",
        "for tool_name, tool_path in tools.items():\n",
        "    print(f\"Carregando dados de {tool_name}...\")\n",
        "    all_data[tool_name] = load_tool_data(tool_path)\n",
        "    print(f\"  - {tool_name} carregado com sucesso\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Cálculo de Métricas por Ferramenta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcula todas as métricas\n",
        "results = {}\n",
        "\n",
        "for tool_name, data in all_data.items():\n",
        "    print(f\"\\nCalculando métricas para {tool_name}...\")\n",
        "    \n",
        "    results[tool_name] = {\n",
        "        'feedback_loop': calculate_feedback_loop_metrics(data),\n",
        "        'cognitive_load': calculate_cognitive_load_metrics(data),\n",
        "        'flow': calculate_flow_metrics(data),\n",
        "        'profile': get_profile_metrics(data)\n",
        "    }\n",
        "    \n",
        "    print(f\"  - Métricas calculadas para {tool_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Consolidação dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cria DataFrames para cada categoria de métrica\n",
        "feedback_loop_df = pd.DataFrame({tool: results[tool]['feedback_loop'] for tool in results}).T\n",
        "cognitive_load_df = pd.DataFrame({tool: results[tool]['cognitive_load'] for tool in results}).T\n",
        "flow_df = pd.DataFrame({tool: results[tool]['flow'] for tool in results}).T\n",
        "profile_df = pd.DataFrame({tool: results[tool]['profile'] for tool in results}).T\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADOS DA ANÁLISE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualização: Métricas de Feedback Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n### MÉTRICAS DE FEEDBACK LOOP ###\")\n",
        "print(feedback_loop_df)\n",
        "\n",
        "# Visualização\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Métricas de Feedback Loop por Ferramenta', fontsize=16)\n",
        "\n",
        "# Tempo até merge\n",
        "feedback_loop_df['tempo_ate_merge_hours_mean'].plot(kind='bar', ax=axes[0,0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0,0].set_title('Tempo Médio até Merge (horas)')\n",
        "axes[0,0].set_ylabel('Horas')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Número de revisões\n",
        "feedback_loop_df['numero_revisoes_mean'].plot(kind='bar', ax=axes[0,1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0,1].set_title('Número Médio de Revisões')\n",
        "axes[0,1].set_ylabel('Revisões')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Comentários da ferramenta\n",
        "feedback_loop_df['comentarios_ferramenta_mean'].plot(kind='bar', ax=axes[1,0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[1,0].set_title('Média de Comentários por PR')\n",
        "axes[1,0].set_ylabel('Comentários')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Tempo até primeira revisão\n",
        "feedback_loop_df['tempo_primeira_revisao_hours_mean'].plot(kind='bar', ax=axes[1,1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[1,1].set_title('Tempo Médio até Primeira Revisão (horas)')\n",
        "axes[1,1].set_ylabel('Horas')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feedback_loop_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualização: Métricas de Cognitive Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n### MÉTRICAS DE COGNITIVE LOAD ###\")\n",
        "print(cognitive_load_df)\n",
        "\n",
        "# Visualização\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Métricas de Cognitive Load por Ferramenta', fontsize=16)\n",
        "\n",
        "# Conventional commits\n",
        "cognitive_load_df['conventional_commits_total'].plot(kind='bar', ax=axes[0,0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0,0].set_title('Total de Conventional Commits')\n",
        "axes[0,0].set_ylabel('Commits')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Arquivos modificados\n",
        "cognitive_load_df['arquivos_modificados_mean'].plot(kind='bar', ax=axes[0,1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0,1].set_title('Média de Arquivos Modificados por PR')\n",
        "axes[0,1].set_ylabel('Arquivos')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Code churn\n",
        "cognitive_load_df['code_churn_mean'].plot(kind='bar', ax=axes[1,0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[1,0].set_title('Média de Code Churn (commits por PR)')\n",
        "axes[1,0].set_ylabel('Commits')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Issues delta\n",
        "cognitive_load_df['issues_delta'].plot(kind='bar', ax=axes[1,1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[1,1].set_title('Delta de Issues (Fechadas - Abertas)')\n",
        "axes[1,1].set_ylabel('Issues')\n",
        "axes[1,1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cognitive_load_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualização: Métricas de Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n### MÉTRICAS DE FLOW ###\")\n",
        "print(flow_df)\n",
        "\n",
        "# Visualização\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Métricas de Flow por Ferramenta', fontsize=16)\n",
        "\n",
        "# Total de PRs\n",
        "flow_df['total_prs'].plot(kind='bar', ax=axes[0,0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0,0].set_title('Total de PRs')\n",
        "axes[0,0].set_ylabel('PRs')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Merge rate\n",
        "flow_df['merge_rate'].plot(kind='bar', ax=axes[0,1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0,1].set_title('Taxa de Merge')\n",
        "axes[0,1].set_ylabel('Taxa (0-1)')\n",
        "axes[0,1].set_ylim([0, 1])\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Tempo entre commits\n",
        "flow_df['tempo_entre_commits_mean_hours'].plot(kind='bar', ax=axes[1,0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[1,0].set_title('Tempo Médio entre Commits (horas)')\n",
        "axes[1,0].set_ylabel('Horas')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Tempo até merge\n",
        "flow_df['tempo_ate_merge_mean_hours'].plot(kind='bar', ax=axes[1,1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[1,1].set_title('Tempo Médio até Merge (horas)')\n",
        "axes[1,1].set_ylabel('Horas')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('flow_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Perfil de Projetos e Desenvolvedores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n### PERFIL DE PROJETOS E DESENVOLVEDORES ###\")\n",
        "print(profile_df)\n",
        "\n",
        "# Visualização\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Perfil de Projetos e Desenvolvedores', fontsize=16)\n",
        "\n",
        "# Número de desenvolvedores\n",
        "profile_df['num_developers'].plot(kind='bar', ax=axes[0], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0].set_title('Número de Desenvolvedores')\n",
        "axes[0].set_ylabel('Desenvolvedores')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Total de stars\n",
        "profile_df['total_stars'].plot(kind='bar', ax=axes[1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[1].set_title('Total de Stars nos Repositórios')\n",
        "axes[1].set_ylabel('Stars')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Linguagens únicas\n",
        "profile_df['unique_languages'].plot(kind='bar', ax=axes[2], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[2].set_title('Número de Linguagens Únicas')\n",
        "axes[2].set_ylabel('Linguagens')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('profile_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Exportação dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2pbE-Or-96DD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOpLhE+4RrUNwp0p0cyGg9D",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
